{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c66bbd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ceb513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from open_clip import get_tokenizer\n",
    "from open_clip.transform import image_transform_v2, PreprocessCfg\n",
    "import webdataset as wds\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import Imagenette, ImageFolder, DTD\n",
    "import torch.nn as nn\n",
    "\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer, create_model_and_transforms\n",
    "from src.clipn import CLIPNAdapter, AltCLIPNAdapter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy import interpolate\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO\n",
    "\n",
    "# Allows loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc57649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "checkpoint = torch.load(\"/repo/ALTMEDCLIPN/checkpoints/PMC_model_cosine/latest_checkpoint.pth\", weights_only=False)[\"model_state_dict\"]\n",
    "\n",
    "model = AltCLIPNAdapter(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    \"text.transformer.encoder\"\n",
    ")\n",
    "\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c15f00a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available MedMNIST 2D datasets:\n",
      "PathMNIST\n",
      "multi-class\n",
      "{'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n",
      "\n",
      "ChestMNIST\n",
      "multi-label, binary-class\n",
      "{'0': 'atelectasis', '1': 'cardiomegaly', '2': 'effusion', '3': 'infiltration', '4': 'mass', '5': 'nodule', '6': 'pneumonia', '7': 'pneumothorax', '8': 'consolidation', '9': 'edema', '10': 'emphysema', '11': 'fibrosis', '12': 'pleural', '13': 'hernia'}\n",
      "\n",
      "DermaMNIST\n",
      "multi-class\n",
      "{'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
      "\n",
      "OCTMNIST\n",
      "multi-class\n",
      "{'0': 'choroidal neovascularization', '1': 'diabetic macular edema', '2': 'drusen', '3': 'normal'}\n",
      "\n",
      "PneumoniaMNIST\n",
      "binary-class\n",
      "{'0': 'normal', '1': 'pneumonia'}\n",
      "\n",
      "BreastMNIST\n",
      "binary-class\n",
      "{'0': 'malignant', '1': 'normal, benign'}\n",
      "\n",
      "BloodMNIST\n",
      "multi-class\n",
      "{'0': 'basophil', '1': 'eosinophil', '2': 'erythroblast', '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)', '4': 'lymphocyte', '5': 'monocyte', '6': 'neutrophil', '7': 'platelet'}\n",
      "\n",
      "TissueMNIST\n",
      "multi-class\n",
      "{'0': 'Collecting Duct, Connecting Tubule', '1': 'Distal Convoluted Tubule', '2': 'Glomerular endothelial cells', '3': 'Interstitial endothelial cells', '4': 'Leukocytes', '5': 'Podocytes', '6': 'Proximal Tubule Segments', '7': 'Thick Ascending Limb'}\n",
      "\n",
      "OrganAMNIST\n",
      "multi-class\n",
      "{'0': 'bladder', '1': 'femur-left', '2': 'femur-right', '3': 'heart', '4': 'kidney-left', '5': 'kidney-right', '6': 'liver', '7': 'lung-left', '8': 'lung-right', '9': 'pancreas', '10': 'spleen'}\n",
      "\n",
      "OrganCMNIST\n",
      "multi-class\n",
      "{'0': 'bladder', '1': 'femur-left', '2': 'femur-right', '3': 'heart', '4': 'kidney-left', '5': 'kidney-right', '6': 'liver', '7': 'lung-left', '8': 'lung-right', '9': 'pancreas', '10': 'spleen'}\n",
      "\n",
      "OrganSMNIST\n",
      "multi-class\n",
      "{'0': 'bladder', '1': 'femur-left', '2': 'femur-right', '3': 'heart', '4': 'kidney-left', '5': 'kidney-right', '6': 'liver', '7': 'lung-left', '8': 'lung-right', '9': 'pancreas', '10': 'spleen'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MEDMNIST_2D_DATASETS = {\n",
    "    'pathmnist': medmnist.PathMNIST,\n",
    "    'chestmnist': medmnist.ChestMNIST,\n",
    "    'dermamnist': medmnist.DermaMNIST,\n",
    "    'octmnist': medmnist.OCTMNIST,\n",
    "    'pneumoniamnist': medmnist.PneumoniaMNIST,\n",
    "    'breastmnist': medmnist.BreastMNIST,\n",
    "    'bloodmnist': medmnist.BloodMNIST,\n",
    "    'tissuemnist': medmnist.TissueMNIST,\n",
    "    'organamnist': medmnist.OrganAMNIST,\n",
    "    'organcmnist': medmnist.OrganCMNIST,\n",
    "    'organsmnist': medmnist.OrganSMNIST\n",
    "}\n",
    "\n",
    "print(\"Available MedMNIST 2D datasets:\")\n",
    "for name in MEDMNIST_2D_DATASETS.keys():\n",
    "    ds_info = INFO[name]\n",
    "    print(ds_info[\"python_class\"])\n",
    "    print(ds_info[\"task\"])\n",
    "    print(ds_info[\"label\"])\n",
    "    print()\n",
    "    # print(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc41c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedMNISTDataset(Dataset):\n",
    "    def __init__(self, dataset_name, split='test', transform=None, class_subset=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_name: Name of the MedMNIST dataset\n",
    "            split: 'train', 'val', or 'test'\n",
    "            transform: Transform to apply to images\n",
    "            class_subset: List of class indices to include (for novel class detection)\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "        # Load the dataset\n",
    "        dataset_class = MEDMNIST_2D_DATASETS[dataset_name]\n",
    "        dataset = dataset_class(split=split, size=224)\n",
    "        \n",
    "        self.images = dataset.imgs\n",
    "        self.labels = dataset.labels.squeeze()\n",
    "        \n",
    "        # Get dataset info\n",
    "        self.info = INFO[dataset_name]\n",
    "        self.task_type = self.info['task'] if 'multi-label' not in self.info['task'] else \"multi-label\"\n",
    "        \n",
    "        # Get class names from dataset info\n",
    "        self.class_to_idx = {name: int(idx) for idx, name in self.info['label'].items()}\n",
    "        self.idx_to_class = {int(idx): name for idx, name in self.info['label'].items()}\n",
    "        self.class_names = [self.idx_to_class[idx] for idx in range(len(self.idx_to_class))]\n",
    "\n",
    "        \n",
    "        # Handle different label formats based on task type\n",
    "        if self.task_type == 'multi-label':\n",
    "            # For multi-label (like ChestMNIST), labels are binary vectors\n",
    "            # Convert to proper format and handle class filtering differently\n",
    "            if class_subset is not None:\n",
    "                # For multi-label OOD, we need to select samples that have \n",
    "                # at least one positive label in the subset\n",
    "                mask = np.any(self.labels[:, class_subset] == 1, axis=1)\n",
    "                self.images = self.images[mask]\n",
    "                self.labels = self.labels[mask]\n",
    "                # Keep only the selected classes in labels\n",
    "                self.labels = self.labels[:, class_subset]\n",
    "                self.class_names = [self.class_names[i] for i in class_subset]\n",
    "                self.n_classes = len(class_subset)\n",
    "            else:\n",
    "                self.n_classes = len(self.labels)\n",
    "        else:\n",
    "            # For single-label tasks (multi-class, binary, ordinal)\n",
    "            if class_subset is not None:\n",
    "                mask = np.isin(self.labels, class_subset)\n",
    "                self.images = self.images[mask]\n",
    "                self.labels = self.labels[mask]\n",
    "                self.class_names = [self.class_names[i] for i in class_subset]\n",
    "                self.n_classes = len(class_subset)\n",
    "            else:\n",
    "                self.n_classes = len(self.labels)\n",
    "        \n",
    "        # Create classes attribute with actual class names for zero-shot\n",
    "        self.classes = self.class_names\n",
    "        \n",
    "        print(f\"Loaded {dataset_name} {split} split:\")\n",
    "        print(f\"  Task type: {self.task_type}\")\n",
    "        print(f\"  Samples: {len(self.images)}\")\n",
    "        print(f\"  Classes: {self.n_classes}\")\n",
    "        print(f\"  Class names: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Convert numpy array to PIL Image\n",
    "        img_array = self.images[idx]\n",
    "        # Handle different channel configurations\n",
    "        if img_array.shape[-1] == 1 or len(img_array.shape) == 2:  # Grayscale\n",
    "            img_array = np.repeat(img_array, 3, axis=-1)  # Convert to RGB\n",
    "        elif img_array.shape[-1] == 3:  # Already RGB\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected number of channels: {img_array.shape[-1]}\")\n",
    "            \n",
    "        # Convert to PIL Image\n",
    "        image = Image.fromarray(img_array.astype(np.uint8))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    # Helper methods for zero-shot evaluation\n",
    "    def get_class_name(self, idx):\n",
    "        \"\"\"Get class name from index\"\"\"\n",
    "        if self.task_type == 'multi-label':\n",
    "            # For multi-label, return list of active class names\n",
    "            if isinstance(idx, (list, np.ndarray)):\n",
    "                return [self.class_names[i] for i, active in enumerate(idx) if active == 1]\n",
    "            else:\n",
    "                return [self.class_names[i] for i in range(len(idx)) if idx[i] == 1]\n",
    "        else:\n",
    "            return self.class_names[idx] if idx < len(self.class_names) else f\"class_{idx}\"\n",
    "    \n",
    "    def is_multi_label(self):\n",
    "        \"\"\"Check if this is a multi-label dataset\"\"\"\n",
    "        return self.task_type == 'multi-label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b0d32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_yes_no_feature(dataset, model, tokenizer, device):\n",
    "    txt = []\n",
    "    N = len(dataset.classes)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    if N:\n",
    "        with open(\"/repo/MEDCLIPN/src/prompt/empty_prompt.txt\") as f:\n",
    "            prompt_lis = f.readlines()\n",
    "        num_prom = len(prompt_lis)\n",
    "    for idx in range(num_prom):\n",
    "        for name in dataset.classes:\n",
    "            txt.append(tokenizer(prompt_lis[idx].replace(\"\\n\", \"\").format(name)).unsqueeze(0))\n",
    "    txt = torch.cat(txt, dim=0)\n",
    "    txt = txt.reshape(num_prom, len(dataset.classes), -1)\n",
    "    text_inputs = txt.to(device)\n",
    "    \n",
    "    text_yes_ttl = torch.zeros(len(dataset.classes), 512).to(device)\n",
    "    text_no_ttl = torch.zeros(len(dataset.classes), 512).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_prom):\n",
    "            text_yes_i = model.encode_text(text_inputs[i], normalize=True)\n",
    "            text_no_i = model.encode_text_no(text_inputs[i])\n",
    "            text_no_i = F.normalize(text_no_i, dim=-1)\n",
    "            \n",
    "            text_yes_ttl += text_yes_i\n",
    "            text_no_ttl += text_no_i\n",
    "            \n",
    "    return F.normalize(text_yes_ttl, dim=-1), F.normalize(text_no_ttl, dim=-1)\n",
    "\n",
    "class ViT_Classifier(torch.nn.Module):\n",
    "    def __init__(self, image_encoder, classification_head_yes, classification_head_no):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        flag = True\n",
    "        self.fc_yes = nn.Parameter(classification_head_yes, requires_grad=flag)    # num_classes  num_feat_dimension\n",
    "        self.fc_no = nn.Parameter(classification_head_no, requires_grad=flag)      # num_classes  num_feat_dimension\n",
    "        self.scale = 100. # this is from the parameter of logit scale in CLIPN\n",
    "        \n",
    "    def set_frozen(self, module):\n",
    "        for module_name in module.named_parameters():\n",
    "            module_name[1].requires_grad = False\n",
    "    def set_learnable(self, module):\n",
    "        for module_name in module.named_parameters():\n",
    "            module_name[1].requires_grad = True\n",
    "            \n",
    "    def forward(self, x):\n",
    "        inputs = self.image_encoder(x)\n",
    "        inputs_norm = F.normalize(inputs, dim=-1)\n",
    "        fc_yes = F.normalize(self.fc_yes, dim=-1)\n",
    "        fc_no = F.normalize(self.fc_no, dim=-1)\n",
    "        \n",
    "        logits_yes = self.scale * inputs_norm @ fc_yes.T \n",
    "        logits_no = self.scale * inputs_norm @ fc_no.T\n",
    "        return logits_yes, logits_no, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed956cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_np = lambda x: x.detach().cpu().numpy()\n",
    "\n",
    "def max_logit_score(logits):\n",
    "    \"\"\"\n",
    "    MaxLogit: Uses the maximum logit value as confidence score\n",
    "    Higher values indicate higher confidence (in-distribution)\n",
    "    \"\"\"\n",
    "    return to_np(torch.max(logits, -1)[0])\n",
    "\n",
    "def msp_score(logits):\n",
    "    \"\"\"\n",
    "    Maximum Softmax Probability (MSP): Uses maximum predicted probability\n",
    "    Higher values indicate higher confidence (in-distribution)\n",
    "    \"\"\"\n",
    "    prob = torch.softmax(logits, -1)\n",
    "    return to_np(torch.max(prob, -1)[0])\n",
    "\n",
    "def energy_score(logits):\n",
    "    \"\"\"\n",
    "    Energy Score: Uses log-sum-exp of logits as energy\n",
    "    Higher values indicate higher confidence (in-distribution)\n",
    "    \"\"\"\n",
    "    return to_np(torch.logsumexp(logits, -1))\n",
    "\n",
    "def ctw_score(logits, logits_no):\n",
    "    \"\"\"\n",
    "    Class-wise Temperature Weighting (CTW) - CLIPN specific method\n",
    "    Uses the yes/no probability for the predicted class\n",
    "    Higher values indicate higher confidence (in-distribution)\n",
    "    \"\"\"\n",
    "    idex = torch.argmax(logits, -1).unsqueeze(-1)\n",
    "    yesno = torch.softmax(torch.cat([logits.unsqueeze(-1), logits_no.unsqueeze(-1)], -1), dim=-1)[:, :, 0]\n",
    "    yesno_s = torch.gather(yesno, dim=1, index=idex)\n",
    "    return to_np(yesno_s)\n",
    "\n",
    "def atd_score(logits, logits_no):\n",
    "    \"\"\"\n",
    "    Attention-based Temperature Distribution (ATD) - CLIPN specific method\n",
    "    Weighted average of yes/no probabilities using softmax weights\n",
    "    Higher values indicate higher confidence (in-distribution)\n",
    "    \"\"\"\n",
    "    yesno = torch.softmax(torch.cat([logits.unsqueeze(-1), logits_no.unsqueeze(-1)], -1), dim=-1)[:, :, 0]\n",
    "    return to_np((yesno * torch.softmax(logits, -1)).sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ebdfa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive evaluation...\n",
      "This will evaluate:\n",
      "1. Classification accuracy on each dataset\n",
      "2. Semantic shift detection (novel classes within datasets)\n",
      "3. Modality shift detection (cross-dataset OOD)\n",
      "\n",
      "Running evaluation...\n",
      "================================================================================\n",
      "COMPREHENSIVE MEDMNIST CLIPN EVALUATION\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "SCENARIO 1: ORIGINAL DATASET CLASSIFICATION ACCURACY\n",
      "============================================================\n",
      "\n",
      "--- Evaluating PATHMNIST ---\n",
      "Loaded pathmnist test split:\n",
      "  Task type: multi-class\n",
      "  Samples: 7180\n",
      "  Classes: 7180\n",
      "  Class names: ['adipose', 'background', 'debris', 'lymphocytes', 'mucus', 'smooth muscle', 'normal colon mucosa', 'cancer-associated stroma', 'colorectal adenocarcinoma epithelium']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating classification: 100%|██████████| 225/225 [00:22<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for pathmnist:\n",
      "  accuracy: 0.3462\n",
      "\n",
      "--- Evaluating CHESTMNIST ---\n",
      "Loaded chestmnist test split:\n",
      "  Task type: multi-label\n",
      "  Samples: 22433\n",
      "  Classes: 22433\n",
      "  Class names: ['atelectasis', 'cardiomegaly', 'effusion', 'infiltration', 'mass', 'nodule', 'pneumonia', 'pneumothorax', 'consolidation', 'edema', 'emphysema', 'fibrosis', 'pleural', 'hernia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating classification: 100%|██████████| 702/702 [01:10<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for chestmnist:\n",
      "  micro_auc: 0.5699\n",
      "  macro_auc: 0.5338\n",
      "\n",
      "--- Evaluating DERMAMNIST ---\n",
      "Loaded dermamnist test split:\n",
      "  Task type: multi-class\n",
      "  Samples: 2005\n",
      "  Classes: 2005\n",
      "  Class names: ['actinic keratoses and intraepithelial carcinoma', 'basal cell carcinoma', 'benign keratosis-like lesions', 'dermatofibroma', 'melanoma', 'melanocytic nevi', 'vascular lesions']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating classification: 100%|██████████| 63/63 [00:06<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for dermamnist:\n",
      "  accuracy: 0.4494\n",
      "\n",
      "--- Evaluating OCTMNIST ---\n",
      "Loaded octmnist test split:\n",
      "  Task type: multi-class\n",
      "  Samples: 1000\n",
      "  Classes: 1000\n",
      "  Class names: ['choroidal neovascularization', 'diabetic macular edema', 'drusen', 'normal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating classification: 100%|██████████| 32/32 [00:03<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for octmnist:\n",
      "  accuracy: 0.2500\n",
      "\n",
      "--- Evaluating PNEUMONIAMNIST ---\n",
      "Loaded pneumoniamnist test split:\n",
      "  Task type: binary-class\n",
      "  Samples: 624\n",
      "  Classes: 624\n",
      "  Class names: ['normal', 'pneumonia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating classification: 100%|██████████| 20/20 [00:02<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for pneumoniamnist:\n",
      "  accuracy: 0.3782\n",
      "\n",
      "--- Evaluating BREASTMNIST ---\n",
      "Loaded breastmnist test split:\n",
      "  Task type: binary-class\n",
      "  Samples: 156\n",
      "  Classes: 156\n",
      "  Class names: ['malignant', 'normal, benign']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating classification: 100%|██████████| 5/5 [00:00<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for breastmnist:\n",
      "  accuracy: 0.7372\n",
      "\n",
      "--- Evaluating BLOODMNIST ---\n",
      "Loaded bloodmnist test split:\n",
      "  Task type: multi-class\n",
      "  Samples: 3421\n",
      "  Classes: 3421\n",
      "  Class names: ['basophil', 'eosinophil', 'erythroblast', 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)', 'lymphocyte', 'monocyte', 'neutrophil', 'platelet']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating classification: 100%|██████████| 107/107 [00:10<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bloodmnist:\n",
      "  accuracy: 0.1432\n",
      "\n",
      "--- Evaluating TISSUEMNIST ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 351\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Modality shift detection (cross-dataset OOD)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 351\u001b[0m classification_results, semantic_results, modality_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_comprehensive_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 185\u001b[0m, in \u001b[0;36mrun_comprehensive_evaluation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Evaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMedMNISTDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Create classifier with dataset's classes\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     text_yes, text_no \u001b[38;5;241m=\u001b[39m merge_yes_no_feature(dataset, model, tokenizer, device)\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mMedMNISTDataset.__init__\u001b[0;34m(self, dataset_name, split, transform, class_subset)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m     14\u001b[0m dataset_class \u001b[38;5;241m=\u001b[39m MEDMNIST_2D_DATASETS[dataset_name]\n\u001b[0;32m---> 15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mimgs\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/miniconda/envs/ALTMEDCLIPN/lib/python3.10/site-packages/medmnist/dataset.py:76\u001b[0m, in \u001b[0;36mMedMNIST.__init__\u001b[0;34m(self, split, transform, target_transform, download, as_rgb, root, size, mmap_mode)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_rgb \u001b[38;5;241m=\u001b[39m as_rgb\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[43mnpz_file\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m npz_file[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/ALTMEDCLIPN/lib/python3.10/site-packages/numpy/lib/npyio.py:256\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/miniconda/envs/ALTMEDCLIPN/lib/python3.10/site-packages/numpy/lib/format.py:831\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m             read_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_read_count, count \u001b[38;5;241m-\u001b[39m i)\n\u001b[1;32m    830\u001b[0m             read_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(read_count \u001b[38;5;241m*\u001b[39m dtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[0;32m--> 831\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    832\u001b[0m             array[i:i\u001b[38;5;241m+\u001b[39mread_count] \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfrombuffer(data, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    833\u001b[0m                                                      count\u001b[38;5;241m=\u001b[39mread_count)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m~/miniconda/envs/ALTMEDCLIPN/lib/python3.10/site-packages/numpy/lib/format.py:966\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m    968\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m size:\n",
      "File \u001b[0;32m~/miniconda/envs/ALTMEDCLIPN/lib/python3.10/zipfile.py:930\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 930\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/miniconda/envs/ALTMEDCLIPN/lib/python3.10/zipfile.py:1006\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_type \u001b[38;5;241m==\u001b[39m ZIP_DEFLATED:\n\u001b[1;32m   1005\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[0;32m-> 1006\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39meof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m                  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def maybe_dictionarize(batch):\n",
    "    if isinstance(batch, dict):\n",
    "        return batch\n",
    "\n",
    "    if len(batch) == 2:\n",
    "        batch = {'images': batch[0], 'labels': batch[1]}\n",
    "    elif len(batch) == 3:\n",
    "        batch = {'images': batch[0], 'labels': batch[1], 'metadata': batch[2]}\n",
    "    else:\n",
    "        raise ValueError(f'Unexpected number of elements: {len(batch)}')\n",
    "\n",
    "    return batch\n",
    "\n",
    "def cal_auc_fpr(ind_conf, ood_conf):\n",
    "    conf = np.concatenate((ind_conf, ood_conf))\n",
    "    ind_indicator = np.concatenate((np.ones_like(ind_conf), np.zeros_like(ood_conf)))\n",
    "    auroc = metrics.roc_auc_score(ind_indicator, conf)\n",
    "    fpr,tpr,thresh = roc_curve(ind_indicator, conf, pos_label=1)\n",
    "    fpr = float(interpolate.interp1d(tpr, fpr)(0.95))\n",
    "    return auroc, fpr\n",
    "\n",
    "def evaluate_classification_accuracy(dataset_loader, model, task_type):\n",
    "    \"\"\"\n",
    "    Evaluate classification accuracy on original dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataset_loader, desc=\"Evaluating classification\"):\n",
    "            batch = maybe_dictionarize(batch)\n",
    "            inputs = batch[\"images\"].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits, _, _ = model(inputs)\n",
    "            \n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_labels.append(labels.detach().cpu())\n",
    "    \n",
    "    all_logits = torch.cat(all_logits)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    \n",
    "    if task_type == 'multi-label':\n",
    "        # Multi-label: use micro-averaged AUC\n",
    "        probs = torch.sigmoid(all_logits)\n",
    "        try:\n",
    "            auc_micro = roc_auc_score(to_np(all_labels), to_np(probs), average='micro')\n",
    "            auc_macro = roc_auc_score(to_np(all_labels), to_np(probs), average='macro')\n",
    "            return {'micro_auc': auc_micro, 'macro_auc': auc_macro}\n",
    "        except ValueError:\n",
    "            # Handle case where some classes are missing\n",
    "            return {'micro_auc': 0.0, 'macro_auc': 0.0}\n",
    "    else:\n",
    "        # Single-label: use accuracy\n",
    "        preds = torch.argmax(all_logits, dim=1)\n",
    "        accuracy = accuracy_score(to_np(all_labels), to_np(preds))\n",
    "        return {'accuracy': accuracy}\n",
    "\n",
    "def evaluate_ood_detection(id_loader, ood_loaders, model, methods=['MSP', 'MaxLogit', 'Energy', 'CTW', 'ATD']):\n",
    "    \"\"\"\n",
    "    Evaluate OOD detection performance\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Collect ID scores\n",
    "    id_scores = {method: [] for method in methods}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(id_loader, desc=\"Collecting ID scores\"):\n",
    "            batch = maybe_dictionarize(batch)\n",
    "            inputs = batch[\"images\"].to(device)\n",
    "            \n",
    "            logits, logits_no, _ = model(inputs)\n",
    "            \n",
    "            if 'MaxLogit' in methods:\n",
    "                id_scores['MaxLogit'].extend(max_logit_score(logits))\n",
    "            if 'MSP' in methods:\n",
    "                id_scores['MSP'].extend(msp_score(logits))\n",
    "            if 'Energy' in methods:\n",
    "                id_scores['Energy'].extend(energy_score(logits))\n",
    "            if 'CTW' in methods:\n",
    "                id_scores['CTW'].extend(ctw_score(logits, logits_no))\n",
    "            if 'ATD' in methods:\n",
    "                id_scores['ATD'].extend(atd_score(logits, logits_no))\n",
    "    \n",
    "    # Collect OOD scores and calculate metrics\n",
    "    results = []\n",
    "    \n",
    "    for ood_name, ood_loader in ood_loaders.items():\n",
    "        ood_scores = {method: [] for method in methods}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(ood_loader, desc=f\"Collecting OOD scores: {ood_name}\"):\n",
    "                batch = maybe_dictionarize(batch)\n",
    "                inputs = batch[\"images\"].to(device)\n",
    "                \n",
    "                logits, logits_no, _ = model(inputs)\n",
    "                \n",
    "                if 'MaxLogit' in methods:\n",
    "                    ood_scores['MaxLogit'].extend(max_logit_score(logits))\n",
    "                if 'MSP' in methods:\n",
    "                    ood_scores['MSP'].extend(msp_score(logits))\n",
    "                if 'Energy' in methods:\n",
    "                    ood_scores['Energy'].extend(energy_score(logits))\n",
    "                if 'CTW' in methods:\n",
    "                    ood_scores['CTW'].extend(ctw_score(logits, logits_no))\n",
    "                if 'ATD' in methods:\n",
    "                    ood_scores['ATD'].extend(atd_score(logits, logits_no))\n",
    "        \n",
    "        # Calculate metrics for each method\n",
    "        for method in methods:\n",
    "            try:\n",
    "                auc, fpr95 = cal_auc_fpr(id_scores[method], ood_scores[method])\n",
    "                results.append({\n",
    "                    'method': method,\n",
    "                    'ood_dataset': ood_name,\n",
    "                    'auroc': auc,\n",
    "                    'fpr95': fpr95\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating metrics for {method} on {ood_name}: {e}\")\n",
    "                results.append({\n",
    "                    'method': method,\n",
    "                    'ood_dataset': ood_name,\n",
    "                    'auroc': 0.0,\n",
    "                    'fpr95': 1.0\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_semantic_split(dataset_name, held_out_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Create ID/OOD splits for semantic shift detection\n",
    "    \"\"\"\n",
    "    info = INFO[dataset_name]\n",
    "    total_classes = len(info[\"label\"])\n",
    "    \n",
    "    if total_classes < 3:\n",
    "        return None, None\n",
    "    \n",
    "    all_classes = list(range(total_classes))\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    n_held_out = max(1, int(total_classes * held_out_ratio))\n",
    "    held_out_classes = np.random.choice(all_classes, n_held_out, replace=False)\n",
    "    id_classes = [c for c in all_classes if c not in held_out_classes]\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Semantic Split:\")\n",
    "    print(f\"  ID classes: {id_classes} ({len(id_classes)} classes)\")\n",
    "    print(f\"  OOD classes: {held_out_classes.tolist()} ({len(held_out_classes)} classes)\")\n",
    "    \n",
    "    return id_classes, held_out_classes.tolist()\n",
    "\n",
    "def run_comprehensive_evaluation():\n",
    "    \"\"\"\n",
    "    Run all three evaluation scenarios\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Remove retinamnist as requested\n",
    "    eval_datasets = [name for name in MEDMNIST_2D_DATASETS.keys() if name != 'retinamnist']\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE MEDMNIST CLIPN EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ===== SCENARIO 1: ORIGINAL DATASET CLASSIFICATION =====\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SCENARIO 1: ORIGINAL DATASET CLASSIFICATION ACCURACY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    classification_results = []\n",
    "    \n",
    "    for dataset_name in eval_datasets:\n",
    "        print(f\"\\n--- Evaluating {dataset_name.upper()} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Load dataset\n",
    "            dataset = MedMNISTDataset(dataset_name, split='test', transform=preprocess)\n",
    "            \n",
    "            # Create classifier with dataset's classes\n",
    "            text_yes, text_no = merge_yes_no_feature(dataset, model, tokenizer, device)\n",
    "            classifier = ViT_Classifier(model.visual, text_yes, text_no)\n",
    "            \n",
    "            # Create data loader\n",
    "            loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=1, pin_memory=True)\n",
    "            \n",
    "            # Evaluate classification\n",
    "            results = evaluate_classification_accuracy(loader, classifier, dataset.task_type)\n",
    "            results['dataset'] = dataset_name\n",
    "            results['task_type'] = dataset.task_type\n",
    "            results['n_classes'] = len(dataset.classes)\n",
    "            results['n_samples'] = len(dataset)\n",
    "            \n",
    "            classification_results.append(results)\n",
    "            \n",
    "            print(f\"Results for {dataset_name}:\")\n",
    "            for metric, value in results.items():\n",
    "                if metric not in ['dataset', 'task_type', 'n_classes', 'n_samples']:\n",
    "                    print(f\"  {metric}: {value:.4f}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {dataset_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # ===== SCENARIO 2: SEMANTIC SHIFT DETECTION =====\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SCENARIO 2: SEMANTIC SHIFT DETECTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    semantic_results = []\n",
    "    \n",
    "    for dataset_name in eval_datasets:\n",
    "        print(f\"\\n--- Semantic Shift: {dataset_name.upper()} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Create semantic split\n",
    "            id_classes, ood_classes = create_semantic_split(dataset_name)\n",
    "            \n",
    "            if id_classes is None:\n",
    "                print(f\"Skipping {dataset_name}: insufficient classes for split\")\n",
    "                continue\n",
    "            \n",
    "            # Create ID and OOD datasets\n",
    "            id_dataset = MedMNISTDataset(dataset_name, split='test', transform=preprocess, class_subset=id_classes)\n",
    "            ood_dataset = MedMNISTDataset(dataset_name, split='test', transform=preprocess, class_subset=ood_classes)\n",
    "            \n",
    "            # Create classifier with only ID classes\n",
    "            text_yes, text_no = merge_yes_no_feature(id_dataset, model, tokenizer, device)\n",
    "            classifier = ViT_Classifier(model.visual, text_yes, text_no)\n",
    "            \n",
    "            # Create data loaders\n",
    "            id_loader = DataLoader(id_dataset, batch_size=32, shuffle=False, num_workers=1, pin_memory=True)\n",
    "            ood_loaders = {f\"{dataset_name}_semantic_ood\": DataLoader(ood_dataset, batch_size=32, shuffle=False, num_workers=1, pin_memory=True)}\n",
    "            \n",
    "            # Evaluate OOD detection\n",
    "            ood_results = evaluate_ood_detection(id_loader, ood_loaders, classifier)\n",
    "            \n",
    "            for result in ood_results:\n",
    "                result['scenario'] = 'semantic_shift'\n",
    "                result['id_dataset'] = dataset_name\n",
    "                semantic_results.append(result)\n",
    "                \n",
    "            print(\"Semantic Shift Results:\")\n",
    "            for result in ood_results:\n",
    "                print(f\"  {result['method']:<10}: AUROC={result['auroc']:.4f}, FPR95={result['fpr95']:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in semantic shift evaluation for {dataset_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # ===== SCENARIO 3: MODALITY SHIFT DETECTION =====\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SCENARIO 3: MODALITY SHIFT DETECTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    modality_results = []\n",
    "    \n",
    "    for id_dataset_name in eval_datasets:\n",
    "        print(f\"\\n--- Modality Shift: {id_dataset_name.upper()} vs Others ---\")\n",
    "        \n",
    "        try:\n",
    "            # Load ID dataset\n",
    "            id_dataset = MedMNISTDataset(id_dataset_name, split='test', transform=preprocess)\n",
    "            \n",
    "            # Create classifier with ID dataset classes\n",
    "            text_yes, text_no = merge_yes_no_feature(id_dataset, model, tokenizer, device)\n",
    "            classifier = ViT_Classifier(model.visual, text_yes, text_no)\n",
    "            \n",
    "            # Create ID loader\n",
    "            id_loader = DataLoader(id_dataset, batch_size=32, shuffle=False, num_workers=1, pin_memory=True)\n",
    "            \n",
    "            # Create OOD loaders (all other datasets)\n",
    "            ood_loaders = {}\n",
    "            for ood_dataset_name in eval_datasets:\n",
    "                if ood_dataset_name != id_dataset_name:\n",
    "                    try:\n",
    "                        ood_dataset = MedMNISTDataset(ood_dataset_name, split='test', transform=preprocess)\n",
    "                        ood_loaders[ood_dataset_name] = DataLoader(ood_dataset, batch_size=32, shuffle=False, num_workers=1, pin_memory=True)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not load {ood_dataset_name}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            if not ood_loaders:\n",
    "                print(f\"No OOD datasets available for {id_dataset_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Evaluate OOD detection\n",
    "            ood_results = evaluate_ood_detection(id_loader, ood_loaders, classifier)\n",
    "            \n",
    "            for result in ood_results:\n",
    "                result['scenario'] = 'modality_shift'\n",
    "                result['id_dataset'] = id_dataset_name\n",
    "                modality_results.append(result)\n",
    "            \n",
    "            print(f\"Modality Shift Results (ID: {id_dataset_name}):\")\n",
    "            for result in ood_results:\n",
    "                print(f\"  {result['method']:<10} vs {result['ood_dataset']:<15}: AUROC={result['auroc']:.4f}, FPR95={result['fpr95']:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in modality shift evaluation for {id_dataset_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # ===== FINAL SUMMARY =====\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Classification Summary\n",
    "    if classification_results:\n",
    "        print(\"\\n1. Classification Accuracy Summary:\")\n",
    "        for result in classification_results:\n",
    "            print(f\"  {result['dataset']:<15} ({result['task_type']:<12}): \", end=\"\")\n",
    "            if 'accuracy' in result:\n",
    "                print(f\"Accuracy = {result['accuracy']:.4f}\")\n",
    "            else:\n",
    "                print(f\"Micro-AUC = {result['micro_auc']:.4f}, Macro-AUC = {result['macro_auc']:.4f}\")\n",
    "    \n",
    "    # Semantic Shift Summary\n",
    "    if semantic_results:\n",
    "        print(\"\\n2. Semantic Shift Detection Summary (Average AUROC):\")\n",
    "        semantic_df = pd.DataFrame(semantic_results)\n",
    "        avg_results = semantic_df.groupby(['method'])['auroc'].mean().sort_values(ascending=False)\n",
    "        for method, avg_auroc in avg_results.items():\n",
    "            print(f\"  {method:<10}: {avg_auroc:.4f}\")\n",
    "    \n",
    "    # Modality Shift Summary\n",
    "    if modality_results:\n",
    "        print(\"\\n3. Modality Shift Detection Summary (Average AUROC):\")\n",
    "        modality_df = pd.DataFrame(modality_results)\n",
    "        avg_results = modality_df.groupby(['method'])['auroc'].mean().sort_values(ascending=False)\n",
    "        for method, avg_auroc in avg_results.items():\n",
    "            print(f\"  {method:<10}: {avg_auroc:.4f}\")\n",
    "    \n",
    "    return classification_results, semantic_results, modality_results\n",
    "\n",
    "# ===== EXECUTE EVALUATION =====\n",
    "print(\"Starting comprehensive evaluation...\")\n",
    "print(\"This will evaluate:\")\n",
    "print(\"1. Classification accuracy on each dataset\")\n",
    "print(\"2. Semantic shift detection (novel classes within datasets)\")\n",
    "print(\"3. Modality shift detection (cross-dataset OOD)\")\n",
    "print(\"\\nRunning evaluation...\")\n",
    "\n",
    "classification_results, semantic_results, modality_results = run_comprehensive_evaluation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b1e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALTMEDCLIPN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
